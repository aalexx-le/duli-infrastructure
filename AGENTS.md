# Infrastructure Kubernetes - Agent Documentation

## Quick Start

Complete cluster setup with Kubernetes provisioning + Helm deployment:

```bash
# Step 1: Provision Kubernetes cluster with Terraform
cd terraform
terraform apply

# Step 2: Deploy Kubernetes and applications with Ansible
cd ../ansible

# Deploy complete infrastructure and applications to shared cluster
ansible-playbook -i inventories/hosts.ini playbooks/site.yml

# Deploy with verbose output
ansible-playbook -vvv -i inventories/hosts.ini playbooks/site.yml

# Deploy specific phase
ansible-playbook -i inventories/hosts.ini playbooks/kubespray.yml
ansible-playbook -i inventories/hosts.ini playbooks/do_csi_driver.yml
ansible-playbook -i inventories/hosts.ini playbooks/helm_infrastructure.yml
```

**Note:** The cluster is shared for both staging and production environments:
- Both environments use the same Kubernetes cluster
- Applications deploy to separate namespaces: `staging` and `prod`
- Environment-specific configuration is in `helm/values/{app}-values-{environment}.yaml`

## Project Structure

```
infrastructure-kubernetes/
├── terraform/                       # Infrastructure provisioning (single shared cluster)
│   ├── main.tf                      # Single cluster configuration
│   ├── variables.tf                 # Cluster variables
│   ├── outputs.tf                   # Cluster outputs
│   ├── terraform.tfvars             # Cluster configuration values
│   ├── modules/                     # networking, kubernetes-cluster
│   └── templates/inventory.tpl      # Auto-generates ansible inventory
├── ansible/                         # Configuration management
│   ├── playbooks/                   # site.yml, kubespray.yml, helm_*.yml, do_csi_driver.yml
│   ├── inventories/                 # Shared cluster inventory (auto-generated by terraform)
│   │   ├── hosts.ini                # Cluster hosts (auto-generated)
│   │   └── group_vars/all/
│   │       ├── vars.yml             # Service configuration
│   │       ├── versions.yml         # Component versions
│   │       └── vault.yml            # Encrypted secrets (Ansible Vault)
│   ├── roles/                       # Custom ansible roles
│   ├── ansible.cfg                  # Ansible configuration
│   └── .vault_pass                  # Vault password file
├── helm/                            # Helm charts & values (flat structure)
│   ├── backend/                     # Application service (Chart + templates + values)
│   │   ├── Chart.yml
│   │   ├── templates/
│   │   └── values/
│   │       ├── defaults.yaml
│   │       ├── staging.yaml
│   │       └── prod.yaml
│   ├── ai-service/                  # Application service (Chart + templates + values)
│   │   ├── Chart.yml
│   │   ├── templates/
│   │   └── values/
│   ├── scheduler/                   # Application service (Chart + templates + values)
│   │   ├── Chart.yml
│   │   ├── templates/
│   │   └── values/
│   ├── postgresql-ha/               # Infrastructure service (Bitnami)
│   ├── redis/                       # Infrastructure service (Bitnami)
│   ├── rabbitmq/                    # Infrastructure service (Bitnami)
│   ├── postgresql-ha-values.yml.j2  # Infrastructure service values (Ansible)
│   ├── redis-ha-values.yml.j2       # Infrastructure service values (Ansible)
│   ├── rabbitmq-ha-values.yml.j2    # Infrastructure service values (Ansible)
│   ├── cert-manager-values.yml.j2   # GitOps tool values (Ansible)
│   ├── argocd-values.yml.j2         # GitOps tool values (Ansible)
│   └── rancher-values.yml.j2        # GitOps tool values (Ansible)
├── gitops/                          # GitOps configuration
│   └── applications/                # ArgoCD application templates
├── kubespray/                       # Kubernetes cluster installer (vendored)
└── AGENTS.md                        # This file
```

## Orchestration Playbooks

Main entry: `ansible/playbooks/site.yml` (orchestrates all phases)

**Modular Playbooks:**
- `kubespray.yml` - Kubernetes cluster provisioning
- `do_csi_driver.yml` - DigitalOcean Block Storage CSI driver installation
- `helm_infrastructure.yml` - PostgreSQL HA, Redis HA, RabbitMQ HA, Cert-Manager, ArgoCD, Rancher, Application Secrets, and ArgoCD Application definitions
- `helm_ingress.yml` - Ingress configuration

**Execution Order:**
1. `kubespray.yml` - Creates Kubernetes cluster
2. `do_csi_driver.yml` - Installs DigitalOcean CSI driver (creates `do-block-storage` StorageClass)
3. `helm_infrastructure.yml` - Deploys:
   - PostgreSQL HA, Redis HA, RabbitMQ HA (infrastructure services)
   - Cert-Manager (TLS certificate management)
   - ArgoCD (GitOps continuous deployment)
   - Rancher (multi-cluster management)
   - Application Secrets (postgres-credentials, redis-credentials, rabbitmq-credentials)
   - ArgoCD Applications (backend, ai-service, scheduler - automatically synced from Git)
4. `helm_ingress.yml` - Deploys ingress + verification

Each playbook waits for resources to be Ready before importing the next.

**Application Deployment Flow:**
Applications are now managed by ArgoCD using GitOps:
- Source: Helm charts in `helm/charts/{app}`
- Configuration: Static YAML values in `helm/values/{app}-values-{environment}.yaml`
- Orchestration: ArgoCD Application definitions in `gitops/applications/{app}.yml.j2`
- Automatic sync: Changes in Git repository automatically deploy within 3 minutes

## Environment Setup

**Single Shared Cluster Architecture:**
- All environments (staging and production) deploy to the same Kubernetes cluster
- Environment isolation is achieved through separate Kubernetes namespaces
- Infrastructure services (PostgreSQL, Redis, RabbitMQ) are shared across environments
- Applications deploy to environment-specific namespaces with their own configurations

### Cluster Inventory
- Path: `ansible/inventories/`
- Usage: Target hosts for the shared Kubernetes cluster
- Auto-generated by: Terraform (`terraform apply`)
- Configuration: `group_vars/all/vars.yml`
- Secrets: `group_vars/all/vault.yml` (Ansible Vault)

### Staging Environment
- **Namespace:** `staging`
- **Usage:** Development and testing
- **Image Tags:** Latest development versions (e.g., `latest`)
- **Replicas:** Reduced for cost efficiency (1-2 replicas)
- **Resources:** Lower CPU/memory requests and limits
- **Configuration:** `helm/values/{app}-values-staging.yaml`

### Production Environment
- **Namespace:** `prod`
- **Usage:** Production deployments
- **Image Tags:** Specific version tags (e.g., `v1.0.0`)
- **Replicas:** Higher for availability (2-3+ replicas)
- **Resources:** Higher CPU/memory requests and limits
- **Autoscaling:** Enabled for select services
- **Configuration:** `helm/values/{app}-values-prod.yaml`
- **PDB:** Pod Disruption Budgets enforced

### Inventory Configuration

The shared cluster inventory structure:
```
ansible/inventories/
├── hosts.ini                    # Cluster hosts (auto-generated by Terraform)
└── group_vars/all/
    ├── vars.yml                 # Shared service configuration
    ├── versions.yml             # Component versions
    └── vault.yml                # Encrypted secrets (Ansible Vault)
```

**Setup Instructions:**
1. Run `terraform apply` in `terraform/` directory
2. This auto-generates `ansible/inventories/hosts.ini`
3. Copy and customize `vault.yml.example` to `vault.yml`
4. Encrypt with: `ansible-vault encrypt ansible/inventories/group_vars/all/vault.yml`
5. Run: `ansible-playbook -i inventories/hosts.ini playbooks/site.yml`

## Service Configuration

### Deployment Architecture

**Infrastructure Services** (Ansible-managed):
- PostgreSQL HA, Redis HA, RabbitMQ HA (deployed via Ansible)
- Configuration: `ansible/inventories/{env}/group_vars/all/vars.yml`
- Values files: `helm/values/{service}-ha-values.yml.j2`

**GitOps Tools** (Ansible-managed installation, auto-synced via GitOps):
- Cert-Manager (TLS certificates)
- ArgoCD (continuous deployment)
- Rancher (multi-cluster management)
- Configuration: `helm/values/{tool}-values.yml.j2`

**Applications** (ArgoCD-managed GitOps):
- Backend, AI-Service, Scheduler
- Configuration: `helm/values/{app}-values-{environment}.yaml`
- Deployment: `gitops/applications/{app}.yml.j2`
- Automatic sync: Within 3 minutes of Git changes

### Infrastructure Services Details

High-availability services deployed from Bitnami Helm charts (via Ansible):

**PostgreSQL HA:**
- Chart: `bitnami/postgresql-ha`
- Architecture: 1 primary + N replicas with Pgpool connection pooler
- Default: 3 PostgreSQL nodes, 2 Pgpool instances
- Storage: Persistent volumes via `do-block-storage`
- Endpoints:
  - Staging: `postgres-postgresql-ha-pgpool.staging.svc.cluster.local:5432`
  - Production: `postgres-postgresql-ha-pgpool.prod.svc.cluster.local:5432`

**Redis HA:**
- Chart: `bitnami/redis`
- Architecture: Master-slave replication with Sentinel for automatic failover
- Default: 1 master, 2 replicas, 3 Sentinel nodes
- Storage: Persistent volumes via `do-block-storage`

**RabbitMQ HA:**
- Chart: `bitnami/rabbitmq`
- Architecture: Multi-node cluster with Kubernetes peer discovery
- Default: 3 cluster nodes
- Storage: Persistent volumes via `do-block-storage`

### Application Services (ArgoCD-managed)

Applications are deployed and managed by ArgoCD using GitOps. All configuration is version-controlled in Git. Applications are deployed to environment-specific namespaces (`staging` and `prod`) with clean naming (no environment suffix in resource names).

**Backend:**
- Chart: `helm/charts/backend`
- Resource Name: `backend` (deployed to `staging` or `prod` namespace)
- Staging: 1 replica, latest image tag, debug logging, 512Mi memory
- Production: 2 replicas, v1.0.0 image tag, info logging, 1Gi memory, PDB enabled
- Values: `helm/values/backend-values-{staging,prod}.yaml`
- ArgoCD Template: `gitops/applications/backend.yml.j2`
- Namespace-aware endpoints:
  - Staging: Database at `postgres-postgresql-ha-pgpool.staging.svc.cluster.local`
  - Production: Database at `postgres-postgresql-ha-pgpool.prod.svc.cluster.local`

**AI-Service (ML workload):**
- Chart: `helm/charts/ai-service`
- Resource Name: `ai-service` (deployed to `staging` or `prod` namespace)
- Staging: 2 replicas, higher CPU/memory for ML, latest image tag
- Production: 3 replicas, higher resource limits (4Gi memory), autoscaling enabled, v1.0.0 image tag
- Values: `helm/values/ai-service-values-{staging,prod}.yaml`
- ArgoCD Template: `gitops/applications/ai-service.yml.j2`
- Namespace-aware endpoints:
  - Staging: Database at `postgres-postgresql-ha-pgpool.staging.svc.cluster.local`
  - Production: Database at `postgres-postgresql-ha-pgpool.prod.svc.cluster.local`

**Scheduler (n8n):**
- Chart: `helm/charts/scheduler`
- Resource Name: `scheduler` (deployed to `staging` or `prod` namespace)
- Staging: 1 replica, 10Gi persistent storage, debug logging, hostname: `scheduler.staging.local`
- Production: 2 replicas, 20Gi persistent storage, info logging, hostname: `scheduler.prod.local`
- Database: Connects to PostgreSQL HA cluster via init container
- Values: `helm/values/scheduler-values-{staging,prod}.yaml`
- ArgoCD Template: `gitops/applications/scheduler.yml.j2`
- Namespace-aware endpoints:
  - Staging: Database at `postgres-postgresql-ha-pgpool.staging.svc.cluster.local`
  - Production: Database at `postgres-postgresql-ha-pgpool.prod.svc.cluster.local`

**GitOps Workflow:**
1. Update values in `helm/values/{app}-values-{environment}.yaml`
2. Commit to Git repository
3. ArgoCD automatically detects changes and syncs within 3 minutes
4. All changes tracked in Git history for audit trail

### Infrastructure Service Configuration Structure

Each infrastructure service in `vars.yml` follows this structure:

```yaml
infrastructure_services:
  - name: postgres
    chart: bitnami/postgresql-ha
    values_file: postgres-ha-values.yml.j2
    label_selectors:
      - app.kubernetes.io/name=postgresql-ha
    wait_for_pods: true
    config:
      # Authentication
      user: duli_user
      password: "{{ vault_postgres_password }}"
      database: duli_db
      # High Availability
      replicas:
        postgresql: 3
        pgpool: 2
      # Image
      image:
        tag: "15-alpine"
      # Storage
      persistence:
        enabled: true
        size: 50Gi
        storageClass: "{{ storage_class }}"
```

**Note:** Application services (backend, ai-service, scheduler) are no longer configured in `vars.yml`. Their configuration is in `helm/values/{app}-values-{environment}.yaml` and they are deployed by ArgoCD.

## Configuration Management

### ConfigMap and Secret Best Practices

**Critical Security Rules:**

1. **Never put passwords in ConfigMaps** - Always use Secrets
2. **Split connection strings** - Use individual environment variables
3. **Use envFrom for multiple variables** - Cleaner than individual env entries

**ConfigMap Usage:**
- ✅ Non-sensitive configuration (log levels, feature flags, URLs, ports)
- ✅ Environment variables (NODE_ENV, PYTHONUNBUFFERED)
- ❌ Never passwords, API keys, or connection strings with credentials

**Secret Usage:**
- ✅ Passwords and API keys
- ✅ Database credentials
- ✅ TLS certificates
- ✅ Any sensitive data

**Example Pattern:**

```yaml
# ConfigMap (non-sensitive)
apiVersion: v1
kind: ConfigMap
data:
  DATABASE_HOST: "postgres-postgresql-ha-pgpool.staging.svc.cluster.local"
  DATABASE_PORT: "5432"

---
# Secret (sensitive)
apiVersion: v1
kind: Secret
stringData:
  DATABASE_USER: "duli_user"
  DATABASE_PASSWORD: "secure-password"
  DATABASE_NAME: "duli_db"
```

Note: Update `DATABASE_HOST` to use the appropriate namespace:
- Staging: `postgres-postgresql-ha-pgpool.staging.svc.cluster.local`
- Production: `postgres-postgresql-ha-pgpool.prod.svc.cluster.local`

**Scheduler Special Case:**

The scheduler (n8n) requires `DATABASE_URL` as a single connection string. Since n8n is a third-party container, we use an init container to construct `DATABASE_URL` from individual environment variables:

1. Init container reads `DATABASE_USER`, `DATABASE_PASSWORD`, `DATABASE_HOST`, `DATABASE_PORT`, `DATABASE_NAME`
2. Constructs `postgresql://user:password@host:port/database`
3. Writes to shared volume
4. Main container reads and exports as `DATABASE_URL` before starting n8n

This ensures passwords never appear in ConfigMaps.

### Ansible Vault

Sensitive credentials are stored in encrypted vault files:
- Location: `ansible/inventories/{env}/group_vars/all/vault.yml`
- Encryption: Ansible Vault
- Password file: `ansible/.vault_pass`

**Required Vault Variables:**
- `vault_postgres_password` - PostgreSQL database password
- `vault_redis_password` - Redis password
- `vault_rabbitmq_password` - RabbitMQ password
- `vault_do_api_token` - DigitalOcean API token (for CSI driver)

**Create/Edit Vault:**
```bash
cd ansible
ansible-vault edit inventories/group_vars/all/vault.yml
```

## Storage Configuration

### Helm Repositories

The following Helm repositories and registries are used:

- **Bitnami**: `https://charts.bitnami.com/bitnami`
  - Used for: PostgreSQL HA, Redis HA, RabbitMQ HA
  - Added in: `helm_infrastructure.yml`

- **DigitalOcean OCI Registry**: `oci://registry.digitalocean.com/digitalocean/csi-digitalocean`
  - Used for: DigitalOcean CSI driver
  - Installed directly from OCI registry (no repository add needed)
  - Playbook: `do_csi_driver.yml`

### DigitalOcean Block Storage

Kubespray does not natively support DigitalOcean block storage. We install the DigitalOcean CSI driver via Helm:

**Playbook:** `ansible/playbooks/do_csi_driver.yml`

**What it does:**
1. Creates Kubernetes Secret with DO API token
2. Installs CSI driver via Helm from OCI registry (`oci://registry.digitalocean.com/digitalocean/csi-digitalocean`)
3. Waits for controller and node pods to be ready
4. Verifies `do-block-storage` StorageClass is created

**StorageClass:** `do-block-storage`
- Provisioner: `dobs.csi.digitalocean.com`
- Volume binding: WaitForFirstConsumer
- Reclaim policy: Delete

**Required:**
- `vault_do_api_token` in vault.yml
- Get token from: https://cloud.digitalocean.com/account/api/tokens

### Persistent Volumes

All infrastructure services use persistent volumes:
- PostgreSQL: 50Gi per node
- Redis: 10Gi (master + replicas)
- RabbitMQ: 20Gi per node
- Scheduler: 10Gi

Volumes are dynamically provisioned using `do-block-storage` StorageClass.

## Security

### Secret Encryption at Rest

Kubespray supports encrypting Secrets in etcd. To enable:

**Add to `ansible/inventories/{env}/group_vars/all/vars.yml`:**

```yaml
# Enable Secret encryption at rest
kube_encrypt_secret_data: true
kube_encryption_algorithm: "secretbox"  # Options: secretbox, aescbc, aesgcm
kube_encryption_resources: [secrets]
```

**Note:** This must be configured before initial cluster deployment. Enabling on an existing cluster requires re-encrypting all Secrets.

**Algorithm Options:**
- `secretbox` (recommended) - Default, secure, no rotation needed
- `aescbc` - Not recommended (CBC vulnerability)
- `aesgcm` - Requires rotation every 200k writes
- `kms` - Requires external KMS service

### RBAC

Configure Role-Based Access Control for Secret access:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: secret-reader
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    resourceNames: ["app-secret"]
    verbs: ["get", "list"]
```

## Configuration Standards

### Environment Naming and Namespaces
- **Staging Environment:**
  - Kubernetes Namespace: `staging`
  - Application Names: Clean (no suffix) - e.g., `backend`, `ai-service`, `scheduler`
  - Service Endpoints: Use `staging` namespace - e.g., `postgres-postgresql-ha-pgpool.staging.svc.cluster.local`
- **Production Environment:**
  - Kubernetes Namespace: `prod`
  - Application Names: Clean (no suffix) - e.g., `backend`, `ai-service`, `scheduler`
  - Service Endpoints: Use `prod` namespace - e.g., `postgres-postgresql-ha-pgpool.prod.svc.cluster.local`

**Best Practice:** Use separate namespaces for environment isolation combined with Kubernetes standard labels (app.kubernetes.io/*, environment, team, tier) for resource identification and querying.

### File Naming Standards
- YAML files: `.yml` extension
- Ansible playbooks: `.yml`
- Helm charts: `.yml`
- Jinja2 templates: `.yml.j2`

### Helm Charts
Charts are centralized in `helm/charts/` and referenced by Ansible playbooks. Do NOT create duplicate charts in `ansible/roles/`.

**Custom Charts:**
- `backend/` - Backend application
- `ai-service/` - AI service application
- `scheduler/` - n8n scheduler

**Community Charts (Bitnami):**
- `bitnami/postgresql-ha` - PostgreSQL HA
- `bitnami/redis` - Redis HA with Sentinel
- `bitnami/rabbitmq` - RabbitMQ HA

Edit `helm/values/*.yml.j2` for application configuration:
- Image versions
- Resource limits/requests
- Environment variables
- Pod replicas
- Storage configuration

## Command Reference

All commands run from `ansible` directory and use `-i inventories/hosts.ini` for the shared cluster.

### Provision Infrastructure
```bash
cd terraform
terraform plan
terraform apply
```

### Run Full Orchestration (Shared Cluster)
```bash
cd ansible
ansible-playbook -i inventories/hosts.ini playbooks/site.yml
```

### Run Individual Phases (Shared Cluster)
```bash
ansible-playbook -i inventories/hosts.ini playbooks/kubespray.yml
ansible-playbook -i inventories/hosts.ini playbooks/do_csi_driver.yml
ansible-playbook -i inventories/hosts.ini playbooks/helm_infrastructure.yml
```

### Verify Deployment
```bash
kubectl cluster-info
kubectl get nodes

# Check staging environment
kubectl get all -n staging
helm list -n staging
kubectl get pvc -n staging

# Check production environment
kubectl get all -n prod
helm list -n prod
kubectl get pvc -n prod

# Storage
kubectl get storageclass
```

### Check Service Status
```bash
# Infrastructure services - staging
kubectl get pods -n staging -l app.kubernetes.io/name=postgresql-ha
kubectl get pods -n staging -l app.kubernetes.io/name=redis
kubectl get pods -n staging -l app.kubernetes.io/name=rabbitmq

# Application services - staging
kubectl get pods -n staging -l app.kubernetes.io/name=backend
kubectl get pods -n staging -l app.kubernetes.io/name=ai-service
kubectl get pods -n staging -l app.kubernetes.io/name=scheduler

# Infrastructure services - production
kubectl get pods -n prod -l app.kubernetes.io/name=postgresql-ha
kubectl get pods -n prod -l app.kubernetes.io/name=redis
kubectl get pods -n prod -l app.kubernetes.io/name=rabbitmq

# Application services - production
kubectl get pods -n prod -l app.kubernetes.io/name=backend
kubectl get pods -n prod -l app.kubernetes.io/name=ai-service
kubectl get pods -n prod -l app.kubernetes.io/name=scheduler
```

## Logging and Output

**Real-time Streaming Output:**
All task output streams directly to console. Output is also automatically saved to `logs/ansible.log`.

**Verbosity Control:**
```bash
# Standard output (default)
ansible-playbook -i inventories/hosts.ini playbooks/site.yml

# Verbose output
ansible-playbook -v -i inventories/hosts.ini playbooks/site.yml

# Very verbose (includes all debug tasks)
ansible-playbook -vv -i inventories/hosts.ini playbooks/site.yml

# Maximum verbosity (connection debugging)
ansible-playbook -vvv -i inventories/hosts.ini playbooks/site.yml
```

## Troubleshooting

### Playbook Won't Start
```bash
ansible --version
ls ../kubespray/cluster.yml
```

### Kubespray Fails
Check inventory and host configuration:
```bash
ansible all -i inventories/hosts.ini -m ping
```

### CSI Driver Issues
Verify DigitalOcean API token and StorageClass:
```bash
kubectl get secret digitalocean-csi-secret -n kube-system
kubectl get storageclass do-block-storage
kubectl get pods -n kube-system -l app=csi-digitalocean-controller
kubectl get pods -n kube-system -l app=csi-digitalocean-node
```

### Helm Pods Stuck
```bash
# Check staging environment
watch kubectl get pods -n staging
kubectl describe pod -n staging <pod-name>
kubectl logs -n staging <pod-name>

# Check production environment
watch kubectl get pods -n prod
kubectl describe pod -n prod <pod-name>
kubectl logs -n prod <pod-name>
```

### Persistent Volume Issues
```bash
kubectl get pv

# Check staging environment
kubectl get pvc -n staging
kubectl describe pvc <pvc-name> -n staging

# Check production environment
kubectl get pvc -n prod
kubectl describe pvc <pvc-name> -n prod
```

### Vault Password Issues
Ensure `.vault_pass` exists in `ansible/` directory and is referenced in `ansible.cfg`:
```bash
ls .vault_pass
```

### Cleanup
Reset cluster:
```bash
ansible-playbook -i inventories/hosts.ini ../kubespray/reset.yml
```

Delete staging environment:
```bash
kubectl delete namespace staging
```

Delete production environment:
```bash
kubectl delete namespace prod
```

## System Requirements

- Ansible 2.17+
- Python 3.8+
- kubectl (latest stable)
- helm v3.0+
- SSH access to target hosts (if remote)
- DigitalOcean API token (for CSI driver)

## Design Principles

- **Pure Ansible**: No shell scripts - all native Ansible tasks
- **Modular**: Independent playbooks for each deployment phase
- **Idempotent**: Safe to run repeatedly without side effects
- **Environment-Aware**: Support for multiple environments via inventories
- **Resilient**: Built-in retries and health checks
- **Observable**: Real-time output streaming + permanent logs
- **Local Execution**: Kubespray runs locally, no remote SSH required
- **Templated Configuration**: Jinja2 variables for dynamic Helm values
- **Data-Driven**: Service configuration in `vars.yml` with loop-based deployment
- **High Availability**: All infrastructure services use HA configurations
- **Security First**: Secrets in Vault, ConfigMaps for non-sensitive data only

## High Availability Architecture

### PostgreSQL HA
- **Primary + Replicas**: 1 primary, 2+ replicas for read scaling
- **Pgpool**: Connection pooler with 2+ instances for load balancing
- **Automatic Failover**: Repmgr handles primary promotion
- **Endpoint**: Always use Pgpool endpoint for connections

### Redis HA
- **Master-Slave**: 1 master, 2+ replicas for read scaling
- **Sentinel**: 3+ Sentinel nodes for automatic failover
- **Quorum**: Majority of Sentinels required for failover decisions

### RabbitMQ HA
- **Cluster**: 3+ nodes in a cluster
- **Peer Discovery**: Kubernetes-based automatic peer discovery
- **Queue Mirroring**: Queues replicated across cluster nodes

## Architecture: IaC vs GitOps

This project follows a **hybrid architecture** separating Infrastructure as Code from GitOps continuous deployment:

### IaC Layer (Bootstrap via Ansible)
**What:** Infrastructure services and platform tools installation
**When:** Runs once during initial cluster setup
**Tools:** Ansible playbooks
**Scope:** Infrastructure services + GitOps platform bootstrap

**Services in IaC:**
- PostgreSQL HA, Redis HA, RabbitMQ HA (stateful data services)
- Cert-Manager (TLS certificate management)
- ArgoCD (GitOps continuous deployment tool itself)
- Rancher (multi-cluster management)

**Why IaC?**
- Stateful services that require initial database setup
- Platform tools that rarely change after deployment
- Bootstrap phase that runs once
- DevOps team responsibility

### GitOps Layer (Continuous via ArgoCD)
**What:** Application deployment and management
**When:** Runs 24/7 watching Git repository for changes
**Tools:** ArgoCD (continuous deployment)
**Scope:** Application services (backend, ai-service, scheduler)

**Services in GitOps:**
- Backend API service
- AI Service (ML workload)
- Scheduler (n8n)

**Why GitOps?**
- Stateless applications that change frequently
- Developer workflow (commit → auto-deploy)
- Declarative source of truth in Git
- Application team responsibility

### Deployment Flow
```
STEP 1: IaC Bootstrap (Ansible - runs once)
  ├─ ansible-playbook site.yml
  ├─ Deploys: K8s, CSI driver, PostgreSQL, Redis, RabbitMQ
  ├─ Installs: Cert-Manager, ArgoCD, Rancher
  └─ Creates: ArgoCD Application CRDs (bootstrap)

     ↓

STEP 2: GitOps Management (ArgoCD - continuous 24/7)
  ├─ ArgoCD watches: https://github.com/duli-ai/gitops-repo
  ├─ When developers push: automatic sync to cluster
  ├─ Manages: backend, ai-service, scheduler deployments
  └─ Result: applications deployed from Git (source of truth)
```

### Key Design Principle
**Separation of Concerns:**
- IaC handles infrastructure bootstrap (once)
- GitOps handles application deployment (continuous)
- ArgoCD itself is installed by Ansible, then manages applications
- This is **industry best practice** (Netflix, Stripe, etc. follow this pattern)

**Self-Managing GitOps (NOT Recommended):**
Some teams use an advanced pattern where ArgoCD manages itself. This is **not recommended** for most organizations because:
- ❌ ArgoCD can accidentally break itself
- ❌ Harder to recover from failures
- ❌ Requires GitOps expertise
- ✅ Only use if your organization mandates "everything in Git"

---

## Helm Directory Structure

The helm directory uses a **flat, self-contained service model** for clear organization:

```
helm/
├── backend/              # Application service (Chart + templates + values)
├── ai-service/           # Application service (Chart + templates + values)
├── scheduler/            # Application service (Chart + templates + values)
├── postgresql-ha/        # Infrastructure service (Bitnami, local)
├── redis/                # Infrastructure service (Bitnami, local)
├── rabbitmq/             # Infrastructure service (Bitnami, local)
├── postgresql-ha-values.yml.j2   # Infrastructure service values (Ansible)
├── redis-ha-values.yml.j2        # Infrastructure service values (Ansible)
├── rabbitmq-ha-values.yml.j2     # Infrastructure service values (Ansible)
├── cert-manager-values.yml.j2    # Platform tool values (Ansible)
├── argocd-values.yml.j2          # Platform tool values (Ansible)
└── rancher-values.yml.j2         # Platform tool values (Ansible)
```

### Design
- **Self-contained services** - Each service folder has Chart, templates, and values together
- **Flat structure** - Single level for easy navigation
- **Separation** - Application services vs infrastructure services at same level
- **Extensibility** - New services follow consistent pattern
- **Local charts** - Infrastructure services stored locally for reproducibility

### Application Service Structure
Each application service (backend, ai-service, scheduler) follows this pattern:
```
{service}/
├── Chart.yml                    # Helm chart definition
├── templates/
│   ├── deployment.yml           # Kubernetes Deployment
│   ├── service.yml              # Kubernetes Service
│   ├── configmap.yml            # ConfigMap for non-sensitive config
│   ├── hpa.yml                  # Horizontal Pod Autoscaler
│   ├── _helpers.tpl             # Helm template helpers
│   └── ...
└── values/
    ├── defaults.yaml            # Default values
    ├── staging.yaml             # Staging overrides
    └── prod.yaml                # Production overrides
```

### Infrastructure Service Structure
Infrastructure services (postgresql-ha, redis, rabbitmq) are Bitnami charts stored locally for reproducibility:
```
{service}/
├── Chart.yaml                   # Chart metadata
├── values.yaml                  # Default values
├── templates/                   # Kubernetes templates
├── charts/                       # Subchart dependencies
└── ...                           # Other chart files
```

### Infrastructure Values Configuration
Values for infrastructure services are stored separately as Jinja2 templates for Ansible:
```
postgresql-ha-values.yml.j2      # PostgreSQL HA configuration
redis-ha-values.yml.j2            # Redis HA configuration
rabbitmq-ha-values.yml.j2         # RabbitMQ HA configuration
```

---

## Local Helm Charts

Infrastructure services use **local Helm charts** stored in the repository:

### Charts
```
helm/postgresql-ha/    # Bitnami PostgreSQL HA v16.3.2
helm/redis/            # Bitnami Redis v24.0.0
helm/rabbitmq/         # Bitnami RabbitMQ v16.0.14
```

### Benefits
- **Reproducibility** - Same chart version always deployed
- **Offline capability** - No internet dependency at deployment time
- **Auditability** - Chart changes tracked in Git history
- **Control** - Review changes before deployment

### Ansible Deployment
Infrastructure services reference local charts:
```yaml
# In ansible/playbooks/helm_infrastructure.yml
chart_ref: "{{ helm_dir }}/{{ item.chart }}"

# Resolves to:
chart_ref: "/path/to/helm/postgresql-ha"
chart_ref: "/path/to/helm/redis"
chart_ref: "/path/to/helm/rabbitmq"
```

### Updating Charts
To update to a new Bitnami chart version:
```bash
# Download new version
helm pull bitnami/postgresql-ha --untar --version X.Y.Z

# Replace old version
rm -rf helm/postgresql-ha
mv postgresql-ha helm/

# Commit to Git
git add helm/postgresql-ha/
git commit -m "Update PostgreSQL HA to vX.Y.Z"
```

---

## Playbook Idempotency

All playbooks are fully **idempotent** - safe to run repeatedly without side effects:

### CSI Driver Installation
- Checks if StorageClass already exists before downloading/applying
- Second run skips download and application (reports `ok` not `changed`)

### Helm Repository Operations
- Checks if repository already added before running `helm repo add`
- Only reports `changed` if repository was actually added

### Infrastructure Service Deployment
- Checks if release already deployed via `helm list`
- Uses `changed_when` to report accurate task status
- Safe to re-run (idempotent)

### Validation Tasks
- All validation commands use `changed_when: false`
- kubectl queries don't trigger re-deployments

### Example: Idempotent CSI Driver Deployment
```yaml
# First run: Downloads and applies
- name: Check if StorageClass already exists
  kubernetes.core.k8s_info:
    kind: StorageClass
    name: do-block-storage
  register: storageclass_check

- name: Download manifests (only if not exists)
  when: storageclass_check.resources | length == 0
  # ...downloads...

# Second run: Skips everything
# Result: status=ok, changed=false
```

---

## Notes

- Terraform auto-generates `ansible/inventories/hosts.ini` with `terraform apply`
- SSH keys must be in `~/.ssh/duli_{environment}` (portable location)
- Vault password file: `ansible/.vault_pass`
- All Helm charts use local location: `helm/`
- Kubespray is vendored (no nested .git)
- Python interpreter is auto-detected
- DigitalOcean CSI driver must be installed before deploying services with persistent volumes
- All infrastructure services use Bitnami community charts for HA, downloaded locally for reproducibility
- Application services use custom Helm charts
- Service configuration is centralized in `vars.yml` for easy maintenance
- Playbooks are fully idempotent - safe to run multiple times
- Architecture follows IaC + GitOps separation (industry best practice)
