# Infrastructure Kubernetes - Agent Documentation

## Quick Start

Complete cluster setup with Kubernetes provisioning + Helm deployment:

```bash
cd ansible

# Deploy to staging environment
ansible-playbook -i inventories/staging/hosts.ini playbooks/site.yml

# Deploy to production environment
ansible-playbook -i inventories/prod/hosts.ini playbooks/site.yml

# Deploy with verbose output
ansible-playbook -vvv -i inventories/staging/hosts.ini playbooks/site.yml
```

## Project Structure

```
infrastructure-kubernetes/
├── terraform/                       # Infrastructure provisioning
│   ├── environments/                # staging/, prod/
│   ├── modules/                     # networking, kubernetes-cluster
│   └── templates/inventory.tpl      # Auto-generates ansible inventories
├── ansible/                         # Configuration management
│   ├── playbooks/                   # site.yml, kubespray.yml, helm_*.yml, do_csi_driver.yml
│   ├── inventories/                 # staging/, prod/ (auto-generated by terraform)
│   │   └── {env}/group_vars/all/
│   │       ├── vars.yml             # Service configuration
│   │       └── vault.yml           # Encrypted secrets (Ansible Vault)
│   ├── roles/                       # Custom ansible roles
│   ├── ansible.cfg                  # Ansible configuration
│   └── .vault_pass                  # Vault password file
├── helm/                            # Helm charts & values
│   ├── charts/                      # backend, ai-service, scheduler (custom charts)
│   └── values/                      # Jinja2 templated values (*.yml.j2)
│       ├── postgres-ha-values.yml.j2
│       ├── redis-ha-values.yml.j2
│       ├── rabbitmq-ha-values.yml.j2
│       ├── backend-values.yml.j2
│       ├── ai-service-values.yml.j2
│       ├── scheduler-values.yml.j2
│       └── ingress.yml.j2
├── kubespray/                       # Kubernetes cluster installer (vendored)
└── AGENTS.md                        # This file
```

## Orchestration Playbooks

Main entry: `ansible/playbooks/site.yml` (orchestrates all phases)

**Modular Playbooks:**
- `kubespray.yml` - Kubernetes cluster provisioning
- `do_csi_driver.yml` - DigitalOcean Block Storage CSI driver installation
- `helm_infrastructure.yml` - PostgreSQL HA, Redis HA, RabbitMQ HA
- `helm_applications.yml` - Backend, AI-Service, Scheduler
- `helm_ingress.yml` - Ingress configuration

**Execution Order:**
1. `kubespray.yml` - Creates Kubernetes cluster
2. `do_csi_driver.yml` - Installs DigitalOcean CSI driver (creates `do-block-storage` StorageClass)
3. `helm_infrastructure.yml` - Deploys infrastructure services (databases, cache, message broker)
4. `helm_applications.yml` - Deploys application services
5. `helm_ingress.yml` - Deploys ingress + verification

Each playbook waits for resources to be Ready before importing the next.

## Environment Setup

Two environments are supported via inventory selection:

### Staging
- Path: `ansible/inventories/staging/`
- Usage: Development and testing
- Scalability: Lower resource allocation
- Image Tags: Latest development versions
- Replicas: Reduced for cost efficiency

### Production
- Path: `ansible/inventories/prod/`
- Usage: Production deployments
- Scalability: Higher resource allocation and replicas
- Image Tags: Specific version tags (e.g., v1.0.0)
- Replicas: Increased for high availability

### Inventory Configuration

Each environment has its own inventory directory with:
- `hosts.ini` - Target hosts (auto-generated by Terraform)
- `group_vars/all/vars.yml` - Environment-specific service configuration
- `group_vars/all/vault.yml` - Encrypted secrets (Ansible Vault)

**Create New Environment:**
1. Create `ansible/inventories/your-env/` directory
2. Copy `staging/hosts.ini` and customize
3. Copy `staging/group_vars/` and update variables
4. Run: `ansible-playbook -i inventories/your-env/hosts.ini playbooks/site.yml`

## Service Configuration

All services are configured in `ansible/inventories/{env}/group_vars/all/vars.yml` using a data-driven structure.

### Infrastructure Services

High-availability services deployed from Bitnami Helm charts:

**PostgreSQL HA:**
- Chart: `bitnami/postgresql-ha`
- Architecture: 1 primary + N replicas with Pgpool connection pooler
- Default: 3 PostgreSQL nodes, 2 Pgpool instances
- Storage: Persistent volumes via `do-block-storage`
- Endpoint: `postgres-postgresql-ha-pgpool.duli.svc.cluster.local:5432`

**Redis HA:**
- Chart: `bitnami/redis`
- Architecture: Master-slave replication with Sentinel for automatic failover
- Default: 1 master, 2 replicas, 3 Sentinel nodes
- Storage: Persistent volumes via `do-block-storage`

**RabbitMQ HA:**
- Chart: `bitnami/rabbitmq`
- Architecture: Multi-node cluster with Kubernetes peer discovery
- Default: 3 cluster nodes
- Storage: Persistent volumes via `do-block-storage`

### Application Services

Custom Helm charts deployed from `helm/charts/`:

**Backend:**
- Custom chart: `helm/charts/backend`
- Replicas: 3 (staging), 5 (prod)
- Resources: Configurable per environment

**AI-Service:**
- Custom chart: `helm/charts/ai-service`
- Replicas: 2 (staging), 3 (prod)
- Resources: Higher CPU/memory for ML workloads

**Scheduler (n8n):**
- Custom chart: `helm/charts/scheduler`
- Replicas: 1 (staging), 2 (prod)
- Database: Connects to PostgreSQL HA cluster
- Special: Uses init container to construct `DATABASE_URL` from individual env vars

### Service Configuration Structure

Each service in `vars.yml` follows this structure:

```yaml
infrastructure_services:
  - name: postgres
    chart: bitnami/postgresql-ha
    values_file: postgres-ha-values.yml.j2
    label_selectors:
      - app.kubernetes.io/name=postgresql-ha
    wait_for_pods: true
    config:
      # Authentication
      user: duli_user
      password: "{{ vault_postgres_password }}"
      database: duli_db
      # High Availability
      replicas:
        postgresql: 3
        pgpool: 2
      # Image
      image:
        tag: "15-alpine"
      # Storage
      persistence:
        enabled: true
        size: 50Gi
        storageClass: "{{ storage_class }}"
```

## Configuration Management

### ConfigMap and Secret Best Practices

**Critical Security Rules:**

1. **Never put passwords in ConfigMaps** - Always use Secrets
2. **Split connection strings** - Use individual environment variables
3. **Use envFrom for multiple variables** - Cleaner than individual env entries

**ConfigMap Usage:**
- ✅ Non-sensitive configuration (log levels, feature flags, URLs, ports)
- ✅ Environment variables (NODE_ENV, PYTHONUNBUFFERED)
- ❌ Never passwords, API keys, or connection strings with credentials

**Secret Usage:**
- ✅ Passwords and API keys
- ✅ Database credentials
- ✅ TLS certificates
- ✅ Any sensitive data

**Example Pattern:**

```yaml
# ConfigMap (non-sensitive)
apiVersion: v1
kind: ConfigMap
data:
  DATABASE_HOST: "postgres-postgresql-ha-pgpool.duli.svc.cluster.local"
  DATABASE_PORT: "5432"

---
# Secret (sensitive)
apiVersion: v1
kind: Secret
stringData:
  DATABASE_USER: "duli_user"
  DATABASE_PASSWORD: "secure-password"
  DATABASE_NAME: "duli_db"
```

**Scheduler Special Case:**

The scheduler (n8n) requires `DATABASE_URL` as a single connection string. Since n8n is a third-party container, we use an init container to construct `DATABASE_URL` from individual environment variables:

1. Init container reads `DATABASE_USER`, `DATABASE_PASSWORD`, `DATABASE_HOST`, `DATABASE_PORT`, `DATABASE_NAME`
2. Constructs `postgresql://user:password@host:port/database`
3. Writes to shared volume
4. Main container reads and exports as `DATABASE_URL` before starting n8n

This ensures passwords never appear in ConfigMaps.

### Ansible Vault

Sensitive credentials are stored in encrypted vault files:
- Location: `ansible/inventories/{env}/group_vars/all/vault.yml`
- Encryption: Ansible Vault
- Password file: `ansible/.vault_pass`

**Required Vault Variables:**
- `vault_postgres_password` - PostgreSQL database password
- `vault_redis_password` - Redis password
- `vault_rabbitmq_password` - RabbitMQ password
- `vault_do_api_token` - DigitalOcean API token (for CSI driver)

**Create/Edit Vault:**
```bash
cd ansible
ansible-vault edit inventories/staging/group_vars/all/vault.yml
```

## Storage Configuration

### Helm Repositories

The following Helm repositories and registries are used:

- **Bitnami**: `https://charts.bitnami.com/bitnami`
  - Used for: PostgreSQL HA, Redis HA, RabbitMQ HA
  - Added in: `helm_infrastructure.yml`

- **DigitalOcean OCI Registry**: `oci://registry.digitalocean.com/digitalocean/csi-digitalocean`
  - Used for: DigitalOcean CSI driver
  - Installed directly from OCI registry (no repository add needed)
  - Playbook: `do_csi_driver.yml`

### DigitalOcean Block Storage

Kubespray does not natively support DigitalOcean block storage. We install the DigitalOcean CSI driver via Helm:

**Playbook:** `ansible/playbooks/do_csi_driver.yml`

**What it does:**
1. Creates Kubernetes Secret with DO API token
2. Installs CSI driver via Helm from OCI registry (`oci://registry.digitalocean.com/digitalocean/csi-digitalocean`)
3. Waits for controller and node pods to be ready
4. Verifies `do-block-storage` StorageClass is created

**StorageClass:** `do-block-storage`
- Provisioner: `dobs.csi.digitalocean.com`
- Volume binding: WaitForFirstConsumer
- Reclaim policy: Delete

**Required:**
- `vault_do_api_token` in vault.yml
- Get token from: https://cloud.digitalocean.com/account/api/tokens

### Persistent Volumes

All infrastructure services use persistent volumes:
- PostgreSQL: 50Gi per node
- Redis: 10Gi (master + replicas)
- RabbitMQ: 20Gi per node
- Scheduler: 10Gi

Volumes are dynamically provisioned using `do-block-storage` StorageClass.

## Security

### Secret Encryption at Rest

Kubespray supports encrypting Secrets in etcd. To enable:

**Add to `ansible/inventories/{env}/group_vars/all/vars.yml`:**

```yaml
# Enable Secret encryption at rest
kube_encrypt_secret_data: true
kube_encryption_algorithm: "secretbox"  # Options: secretbox, aescbc, aesgcm
kube_encryption_resources: [secrets]
```

**Note:** This must be configured before initial cluster deployment. Enabling on an existing cluster requires re-encrypting all Secrets.

**Algorithm Options:**
- `secretbox` (recommended) - Default, secure, no rotation needed
- `aescbc` - Not recommended (CBC vulnerability)
- `aesgcm` - Requires rotation every 200k writes
- `kms` - Requires external KMS service

### RBAC

Configure Role-Based Access Control for Secret access:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: secret-reader
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    resourceNames: ["app-secret"]
    verbs: ["get", "list"]
```

## Configuration Standards

### Environment Naming
- Staging: `staging`
- Production: `prod`

### File Naming Standards
- YAML files: `.yml` extension
- Ansible playbooks: `.yml`
- Helm charts: `.yml`
- Jinja2 templates: `.yml.j2`

### Helm Charts
Charts are centralized in `helm/charts/` and referenced by Ansible playbooks. Do NOT create duplicate charts in `ansible/roles/`.

**Custom Charts:**
- `backend/` - Backend application
- `ai-service/` - AI service application
- `scheduler/` - n8n scheduler

**Community Charts (Bitnami):**
- `bitnami/postgresql-ha` - PostgreSQL HA
- `bitnami/redis` - Redis HA with Sentinel
- `bitnami/rabbitmq` - RabbitMQ HA

Edit `helm/values/*.yml.j2` for application configuration:
- Image versions
- Resource limits/requests
- Environment variables
- Pod replicas
- Storage configuration

## Command Reference

All commands run from `ansible` directory and require `-i inventories/{env}/hosts.ini`.

### Run Full Orchestration
```bash
ansible-playbook -i inventories/staging/hosts.ini playbooks/site.yml
ansible-playbook -i inventories/prod/hosts.ini playbooks/site.yml
```

### Run Individual Phases
```bash
ansible-playbook -i inventories/staging/hosts.ini playbooks/kubespray.yml
ansible-playbook -i inventories/staging/hosts.ini playbooks/do_csi_driver.yml
ansible-playbook -i inventories/staging/hosts.ini playbooks/helm_infrastructure.yml
ansible-playbook -i inventories/staging/hosts.ini playbooks/helm_applications.yml
ansible-playbook -i inventories/staging/hosts.ini playbooks/helm_ingress.yml
```

### Verify Deployment
```bash
kubectl cluster-info
kubectl get nodes
kubectl get all -n duli
helm list -n duli
kubectl get storageclass
kubectl get pvc -n duli
```

### Check Service Status
```bash
# Infrastructure services
kubectl get pods -n duli -l app.kubernetes.io/name=postgresql-ha
kubectl get pods -n duli -l app.kubernetes.io/name=redis
kubectl get pods -n duli -l app.kubernetes.io/name=rabbitmq

# Application services
kubectl get pods -n duli -l app=backend
kubectl get pods -n duli -l app=ai-service
kubectl get pods -n duli -l app=scheduler
```

## Logging and Output

**Real-time Streaming Output:**
All task output streams directly to console. Output is also automatically saved to `logs/ansible.log`.

**Verbosity Control:**
```bash
# Standard output (default)
ansible-playbook -i inventories/staging/hosts.ini playbooks/site.yml

# Verbose output
ansible-playbook -v -i inventories/staging/hosts.ini playbooks/site.yml

# Very verbose (includes all debug tasks)
ansible-playbook -vv -i inventories/staging/hosts.ini playbooks/site.yml

# Maximum verbosity (connection debugging)
ansible-playbook -vvv -i inventories/staging/hosts.ini playbooks/site.yml
```

## Troubleshooting

### Playbook Won't Start
```bash
ansible --version
ls ../kubespray/cluster.yml
```

### Kubespray Fails
Check inventory and host configuration:
```bash
ansible all -i inventories/staging/hosts.ini -m ping
```

### CSI Driver Issues
Verify DigitalOcean API token and StorageClass:
```bash
kubectl get secret digitalocean-csi-secret -n kube-system
kubectl get storageclass do-block-storage
kubectl get pods -n kube-system -l app=csi-digitalocean-controller
kubectl get pods -n kube-system -l app=csi-digitalocean-node
```

### Helm Pods Stuck
```bash
watch kubectl get pods -n duli
kubectl describe pod -n duli <pod-name>
kubectl logs -n duli <pod-name>
```

### Persistent Volume Issues
```bash
kubectl get pv
kubectl get pvc -n duli
kubectl describe pvc <pvc-name> -n duli
```

### Vault Password Issues
Ensure `.vault_pass` exists in `ansible/` directory and is referenced in `ansible.cfg`:
```bash
ls .vault_pass
```

### Cleanup
Reset cluster:
```bash
ansible-playbook -i inventories/staging/hosts.ini ../kubespray/reset.yml
```

Delete all applications:
```bash
kubectl delete namespace duli
```

## System Requirements

- Ansible 2.17+
- Python 3.8+
- kubectl (latest stable)
- helm v3.0+
- SSH access to target hosts (if remote)
- DigitalOcean API token (for CSI driver)

## Design Principles

- **Pure Ansible**: No shell scripts - all native Ansible tasks
- **Modular**: Independent playbooks for each deployment phase
- **Idempotent**: Safe to run repeatedly without side effects
- **Environment-Aware**: Support for multiple environments via inventories
- **Resilient**: Built-in retries and health checks
- **Observable**: Real-time output streaming + permanent logs
- **Local Execution**: Kubespray runs locally, no remote SSH required
- **Templated Configuration**: Jinja2 variables for dynamic Helm values
- **Data-Driven**: Service configuration in `vars.yml` with loop-based deployment
- **High Availability**: All infrastructure services use HA configurations
- **Security First**: Secrets in Vault, ConfigMaps for non-sensitive data only

## High Availability Architecture

### PostgreSQL HA
- **Primary + Replicas**: 1 primary, 2+ replicas for read scaling
- **Pgpool**: Connection pooler with 2+ instances for load balancing
- **Automatic Failover**: Repmgr handles primary promotion
- **Endpoint**: Always use Pgpool endpoint for connections

### Redis HA
- **Master-Slave**: 1 master, 2+ replicas for read scaling
- **Sentinel**: 3+ Sentinel nodes for automatic failover
- **Quorum**: Majority of Sentinels required for failover decisions

### RabbitMQ HA
- **Cluster**: 3+ nodes in a cluster
- **Peer Discovery**: Kubernetes-based automatic peer discovery
- **Queue Mirroring**: Queues replicated across cluster nodes

## Notes

- Terraform auto-generates `ansible/inventories/{env}/hosts.ini` with `terraform apply`
- SSH keys must be in `~/.ssh/duli_{environment}` (portable location)
- Vault password file: `ansible/.vault_pass`
- All Helm charts use centralized location: `helm/charts/`
- Kubespray is vendored (no nested .git)
- Python interpreter is auto-detected
- DigitalOcean CSI driver must be installed before deploying services with persistent volumes
- All infrastructure services use Bitnami community charts for HA
- Application services use custom Helm charts
- Service configuration is centralized in `vars.yml` for easy maintenance
